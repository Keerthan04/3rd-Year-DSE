{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook merges the concept of Hill Climbing and gym environments(to tackle knows issues of Lab midsem exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis simple linear policy maps the state to an action by taking the dot product with each row of the weight matrix and then selecting the action with the highest resulting value.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "To use hill climbing, you first define a policy parameterized by a set of weights. A simple choice is a linear policy that computes a score for each action from the state. For example, if you use a weight matrix W of shape (3, 2) (three actions and two state features), you can compute:\n",
    "'''\n",
    "def policy(weights,state):\n",
    "    #state is (2,) which is the position and velocity value\n",
    "    #weights is the matrix with dim (action_space.n,state) so here it is (3,2) as we have 3 possible actions\n",
    "    \n",
    "    # so we implement a Linear policy and return the action which has the highest value as the action to take\n",
    "    scores = np.dot(weights,state)\n",
    "    return np.argmax(scores)\n",
    "'''\n",
    "This simple linear policy maps the state to an action by taking the dot product with each row of the weight matrix and then selecting the action with the highest resulting value.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You need a way to judge the quality of a particular policy (set of weights). The evaluation function runs one or several episodes in the environment using the current policy and returns an average performance score (for Mountain Car, a higher score means fewer steps to reach the goal since you receive –1 per time step):\n",
    "'''\n",
    "\n",
    "import gymnasium as gym\n",
    "def evaluate(weights,env,episodes =5):\n",
    "    #this function is to evaluate the performance of the policy by running on episodes\n",
    "    total_reward = 0\n",
    "    for _ in range(episodes):\n",
    "        state,info = env.reset()\n",
    "        done =False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            action = policy(weights,state)#choose action\n",
    "            state,reward,done,truncated,info = env.step(action)\n",
    "            total_reward += reward\n",
    "    return total_reward/episodes#so we return the cumulative avg reward as policy performance issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple hill climbing\n",
    "\n",
    "'''\n",
    "In simple hill climbing, you start with a random set of weights. Then, at each iteration, you generate a single “neighbor” (by adding a small random perturbation to the weights) and update your weights only if the new weights result in a better score.\n",
    "'''\n",
    "def simple_hill_climbing(env,iterations = 50,noise_scale = 0.1):\n",
    "    #init weights \n",
    "    best_weights = np.random.randn(3,2)\n",
    "    best_score = evaluate(best_weights,env)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        #find one neighbor\n",
    "        new_weights = best_weights + np.random.randn(3,2) * noise_scale\n",
    "        new_score = evaluate(new_weights,env)\n",
    "        if new_score > best_score:\n",
    "            best_score,best_weights = new_score,new_weights\n",
    "            print(f\"Iteration {i}: Improved score to {best_score}\")\n",
    "    \n",
    "    return best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In steepest ascent hill climbing, you generate several candidate neighbors at each iteration, evaluate all of them, and choose the best candidate (if it improves over the current best).\n",
    "'''\n",
    "def steepest_hill_climbing(env,iterations = 50,noise_scale = 0.1,num_neighbours = 10):\n",
    "    best_weights = np.random.randn(3,2)\n",
    "    best_score = evaluate(best_weights,env)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        neighbors = [best_weights + np.random.rand(3,2)*noise_scale  for _ in range(num_neighbours)]\n",
    "        scores = [evaluate(neighbors[i],env) for i in range(num_neighbours)]\n",
    "        max_score = np.argmax(scores)\n",
    "        if scores[max_score] > best_score:\n",
    "            best_score,best_weights = scores[max_score],neighbors[max_score]\n",
    "            print(f\"Iteration {i}: Improved score to {best_score}\")\n",
    "    return best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stochastic hill climbing also generates multiple neighbors, but rather than always choosing the best one, it selects one probabilistically according to their performance. This can help in exploring the search space more broadly\n",
    "'''\n",
    "def stochastic_hill_climbing(env, iterations=50, noise_scale=0.1, num_neighbors=10):\n",
    "    best_weights = np.random.randn(3, 2)\n",
    "    best_score = evaluate(best_weights, env)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        neighbors = [best_weights + np.random.randn(3, 2) * noise_scale for _ in range(num_neighbors)]\n",
    "        scores = [evaluate(candidate, env) for candidate in neighbors]\n",
    "        \n",
    "        # Adjust scores to be positive (since rewards are negative in Mountain Car)\n",
    "        min_score = min(scores)\n",
    "        adjusted_scores = [score - min_score + 1e-6 for score in scores]\n",
    "        probabilities = np.array(adjusted_scores) / np.sum(adjusted_scores)\n",
    "        \n",
    "        # Choose one neighbor based on the probability distribution\n",
    "        chosen_index = np.random.choice(range(num_neighbors), p=probabilities)\n",
    "        if scores[chosen_index] > best_score:\n",
    "            best_weights, best_score = neighbors[chosen_index], scores[chosen_index]\n",
    "            print(f\"Iteration {i}: Improved score to {best_score}\")\n",
    "    return best_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Simple Hill Climbing\n",
      "\n",
      "Running Steepest Ascent Hill Climbing\n",
      "\n",
      "Running Stochastic Hill Climbing\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:179: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print(\"Running Simple Hill Climbing\")\n",
    "best_weights_simple = simple_hill_climbing(env)\n",
    "\n",
    "print(\"\\nRunning Steepest Ascent Hill Climbing\")\n",
    "best_weights_steepest = steepest_hill_climbing(env)\n",
    "\n",
    "print(\"\\nRunning Stochastic Hill Climbing\")\n",
    "best_weights_stochastic = stochastic_hill_climbing(env)\n",
    "\n",
    "# Optionally, test the best weights by rendering an episode\n",
    "state,info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "total_reward =0\n",
    "while not done and not truncated:\n",
    "    env.render()\n",
    "    action = policy(best_weights_simple, state)  # or best_weights_steepest / best_weights_stochastic\n",
    "    state, reward, done, truncated,info = env.step(action)\n",
    "    total_reward+=1\n",
    "env.close()\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, exactly. In this approach, you're directly optimizing the weights of your linear policy. The hill climbing algorithm tweaks the weights based on how well the policy performs in the environment (i.e., the cumulative reward over an episode). By iteratively adjusting and evaluating the weights, you're effectively \"improving\" the policy so that it chooses actions leading to better overall performance.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- **Policy:** A linear mapping defined by weights.\n",
    "- **Objective:** Optimize the weights so that the policy's actions yield higher cumulative rewards.\n",
    "- **Method:** Use hill climbing (simple, steepest, or stochastic) to explore weight adjustments and select the best performing ones.\n",
    "\n",
    "This is a common strategy for environments where you can parameterize the decision-making process with a set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, in principle you can apply hill climbing to other Gym environments like CartPole or Taxi. The overall approach remains the same:\n",
    "\n",
    "- **Parameterized Policy:**  \n",
    "  You'll define a policy with parameters (e.g., a weight matrix for a linear policy) that maps states to actions. For instance, for CartPole, where the state has four dimensions, your weight matrix might have shape `(n_actions, 4)`.\n",
    "\n",
    "- **Evaluation:**  \n",
    "  You still evaluate the policy by running full episodes and calculating the cumulative reward. The reward structure might differ—CartPole gives +1 per time step until failure, and Taxi has its own discrete rewards—but the idea is to maximize the total reward.\n",
    "\n",
    "- **Hill Climbing Process:**  \n",
    "  You generate candidate solutions (neighbors) by slightly perturbing the current parameters and update them if they improve the cumulative reward.\n",
    "\n",
    "Keep in mind that the reward signals in different environments can affect how noticeable the improvements are. For example, if CartPole provides a consistently positive reward, even small improvements in survival time might be more obvious than in an environment like Mountain Car where each step carries a penalty.\n",
    "\n",
    "Also, for environments like Taxi, where the state is typically represented as a discrete integer, you might need a different policy representation or feature encoding (e.g., one-hot encoding or a small neural network) to make hill climbing effective.\n",
    "\n",
    "So yes, hill climbing can be applied to other Gym environments, but you may need to adjust your policy representation and evaluation strategy based on the specific characteristics of each environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
