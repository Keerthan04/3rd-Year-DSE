{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dfaf4530",
   "metadata": {},
   "source": [
    "Name: Keerthan Kumar C\n",
    "Section: DSE B\n",
    "Registration Number: 220968002\n",
    "Roll no: 02\n",
    "Batch: B1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe20a6",
   "metadata": {},
   "source": [
    "# **Frozen Lake Environment - Policy Iteration vs. Value Iteration**\n",
    "\n",
    "### **Objective**\n",
    "Learn the optimal policy for the Frozen Lake environment using **Policy Iteration** and **Value Iteration**, and compare their performance.\n",
    "\n",
    "### **Frozen Lake Environment**\n",
    "We use OpenAI Gym's Frozen Lake environment:  \n",
    "ðŸ”— [Frozen Lake - Gym Documentation](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Policy Iteration**\n",
    "### **Parameters:**\n",
    "- **Policy**: 2D array of shape (nS, nA), each cell represents the probability of taking action *a* in state *s*.\n",
    "- **Environment**: Initialized OpenAI Gym environment.\n",
    "- **Discount Factor** (*Î³*): Factor for future rewards.\n",
    "- **Theta**: Convergence threshold for value function updates.\n",
    "- **Max Iterations**: Maximum number of iterations before stopping.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Value Iteration**\n",
    "### **Parameters:**\n",
    "- **Environment**: Initialized OpenAI Gym environment.\n",
    "- **Discount Factor** (*Î³*): Factor for future rewards.\n",
    "- **Theta**: Convergence threshold for value function updates.\n",
    "- **Max Iterations**: Maximum number of iterations before stopping.\n",
    "\n",
    "###  c.  Compare the number of wins, and average return after 1000 episodes and comment on which method performed                   better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69a8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6499eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Discrete(16)\n",
      "action space:  Discrete(4)\n",
      "Number of actions:  4\n",
      "Number of states:  16\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\", is_slippery=True)\n",
    "print('observation space: ',env.observation_space)\n",
    "print('action space: ',env.action_space)\n",
    "num_of_states = env.observation_space.n\n",
    "num_of_actions = env.action_space.n\n",
    "print(\"Number of actions: \",num_of_actions)\n",
    "print(\"Number of states: \",num_of_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b57d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a initial policy with 16*4 board with 0.25 probability for each\n",
    "initial_policy = np.ones((num_of_states,num_of_actions))/num_of_actions\n",
    "#discount_factor\n",
    "gamma = 0.99\n",
    "#threshold to compare between diff iterations the value function value\n",
    "threshold = 1e-8\n",
    "#maximum iterations to do to find the optimal policy\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b64f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ace49",
   "metadata": {},
   "source": [
    "### Policy Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ee88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(policy,env,discount_factor=0.99, theta=1e-8,max_iterations=1000,num_of_actions=4,num_of_states=16):\n",
    "    #performing policy iteration using Dynamic Programming(refer book and slides for understanding)\n",
    "    \"\"\"\n",
    "    Performs Policy Iteration to find an optimal policy for a given environment.\n",
    "\n",
    "    Args:\n",
    "        policy: Initial policy (probabilities of actions in each state)\n",
    "        env: OpenAI Gym environment\n",
    "        discount_factor: Discount rate for future rewards\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Max number of iterations\n",
    "        num_of_states: the number of states default 16(4x4map)\n",
    "        num_of_actions: the number of actions default 4(up,down,left,right)\n",
    "\n",
    "    Returns:\n",
    "        value_function: Optimal state-value function\n",
    "        policy: Optimized policy\n",
    "    \"\"\"\n",
    "    if hasattr(env, 'unwrapped'):\n",
    "        env = env.unwrapped  # Get the raw environment if wrapped\n",
    "    \n",
    "    #initialize the value function values for each state i.e 16 here with inital value of 0\n",
    "    value_functions = np.zeros(num_of_states)\n",
    "    \n",
    "    #loop through max iterations\n",
    "    #policy evaluation-------------\n",
    "    for i in range(max_iterations):\n",
    "        #Policy Evaluation: updation of each state value function with the help of the current policy and bellman equation\n",
    "        #doing this till the theta i.e(the difference between the value function values between current and previous is same or below theta)\n",
    "        while True:\n",
    "            delta = 0 #tracking the values\n",
    "            for state in range(num_of_states):\n",
    "                #storing the value function for delta check\n",
    "                old_value_function = value_functions[state]\n",
    "                #init the new value function with 0\n",
    "                new_value_funtion = 0\n",
    "                for action ,action_prob in enumerate(policy[state]):\n",
    "                    #getting the action and the action probability for the current state from the policy\n",
    "                    for prob, next_state,reward,done in env.P[state][action]:\n",
    "                        #loop through all the transistions to appl the bellman equation to find the new value of the value function of the state\n",
    "                        #applying the bellman equation formula\n",
    "                        new_value_funtion+=action_prob*prob*(reward + discount_factor*value_functions[next_state])\n",
    "                    \n",
    "                #update the value funtion\n",
    "                value_functions[state] = new_value_funtion\n",
    "                #find delta\n",
    "                delta = max(delta,abs(old_value_function-new_value_funtion))\n",
    "            \n",
    "            if delta<theta:\n",
    "                #once we get the delta values for entire all state update check for delta and if change compared to previous is small then we stop\n",
    "                break\n",
    "            #so stop the policy evaluation step and move to policy iteration\n",
    "    \n",
    "    #policy improvement:updating the current policy based on the new value functions\n",
    "    #we say that the policy is stable if the new policy and the current policy are same then stop and return\n",
    "    policy_stable = True\n",
    "    for state in range(num_of_states):\n",
    "        #we are finding the state action value for each state and find the best action for each state, then compare it with the current policy if the actions are same then policy is stable\n",
    "        old_action = np.argmax(policy[state])#find the best action accoring to the current policy\n",
    "        \n",
    "        #define state action value functions all to zeros\n",
    "        action_values = np.zeros(num_of_actions)\n",
    "        #Loop through each action for the state and find the action value function according to the bellman equation\n",
    "        for action in range(num_of_actions):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                #finding the action value on applying bellman equation\n",
    "                action_values[action] += prob * (reward + discount_factor * value_functions[next_state])\n",
    "            \n",
    "            #best action for the state identify based on the action values\n",
    "            best_action = np.argmax(action_values)\n",
    "            new_policy = np.eye(num_of_actions)[best_action]  # Update policy for best action\n",
    "            \n",
    "            if not np.array_equal(new_policy, policy[state]):  # Check if policy changed\n",
    "                policy_stable = False#if policy changed then not stable so again policy iteration\n",
    "            policy[state] = new_policy  # Apply new policy\n",
    "\n",
    "        if policy_stable:  # Stop if policy is stable\n",
    "            break#if stable then finish of value iteration\n",
    "    \n",
    "    #if finish of iterations or policy eval and iteration gave stable policy finish and return the value functions and the optimal policy\n",
    "    return value_functions,policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4944862",
   "metadata": {},
   "source": [
    "### Value Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3795bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env,discount_factor=0.99,theta = 1e-8,max_iterations=1000,num_of_states=16,num_of_actions=4):\n",
    "    #performing value iterations using Dynamic Programming\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to find an optimal policy.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        discount_factor: Discount rate for future rewards\n",
    "        theta: Convergence threshold\n",
    "        max_iterations: Max number of iterations\n",
    "        num_of_states: the number of states default 16(4x4map)\n",
    "        num_of_actions: the number of actions default 4(up,down,left,right)\n",
    "\n",
    "    Returns:\n",
    "        value_function: Optimal state-value function\n",
    "        policy: Optimal policy\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(env, 'unwrapped'):\n",
    "        env = env.unwrapped  # Get raw environment\n",
    "        \n",
    "    #initialize the value function values for each state i.e 16 here with inital value of 0\n",
    "    value_functions = np.zeros(num_of_states)\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        delta = 0 #to track change\n",
    "        for state in range(num_of_states):\n",
    "            #storing the value function for delta check\n",
    "            old_value_function = value_functions[state]\n",
    "            #for each state in value function instead of value evaluation and improvement we find the action value functions for each and update the state value function with max among it  \n",
    "            action_values = np.zeros(num_of_actions)\n",
    "            \n",
    "            #finding action value function value using bellman equation\n",
    "            for action in range(num_of_actions):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    action_values[action] += prob * (reward + discount_factor * value_functions[next_state])\n",
    "            value_functions[state] = np.max(action_values)  # Update state value with maximum of action value\n",
    "            delta = max(delta, abs(old_value_function - value_functions[state]))  # Check for convergence\n",
    "        if delta < theta:  # Stop if change is small\n",
    "            break\n",
    "    \n",
    "    # Derive policy from optimal value function\n",
    "    policy = np.zeros((num_of_states,num_of_actions))/num_of_actions\n",
    "    for state in range(num_of_states):\n",
    "        action_values = np.zeros(num_of_actions)  # Compute action values\n",
    "        for action in range(num_of_actions):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                action_values[action] += prob * (reward + discount_factor * value_functions[next_state])\n",
    "        best_action = np.argmax(action_values)  # Select best action\n",
    "        policy[state] = np.eye(num_of_actions)[best_action]  # One-hot encode policy\n",
    "\n",
    "    return value_functions, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990a076",
   "metadata": {},
   "source": [
    "### Evaluation of the policy obtained on the environment and finding the win ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432b275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Evaluates a given policy by running multiple episodes and calculating win rate and average return.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        policy: Policy to evaluate\n",
    "        num_episodes: Number of episodes to simulate\n",
    "\n",
    "    Returns:\n",
    "        wins: Number of successful episodes (goal reached)\n",
    "        avg_return: Average return per episode\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "    total_return = 0\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset()  # Reset environment\n",
    "        terminated = truncated = False\n",
    "        episode_return = 0\n",
    "        while not (terminated or truncated):  # Run until episode ends\n",
    "            #action choosing from the policy based on probability\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=policy[state])  # Choose action from policy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)  # Take action\n",
    "            episode_return += reward  # Accumulate reward\n",
    "        if reward > 0:  # Check if goal was reached\n",
    "            wins += 1\n",
    "        total_return += episode_return  # Track total return\n",
    "    return wins, total_return / num_episodes  # Return win count and average return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c191c6",
   "metadata": {},
   "source": [
    "### Running both approches and comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5eb9427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Results:\n",
      "Wins: 704/1000 episodes\n",
      "Average Return: 0.704\n",
      "Time Taken: 0.053150 seconds\n",
      "\n",
      "Value Iteration Results:\n",
      "Wins: 742/1000 episodes\n",
      "Average Return: 0.742\n",
      "Time Taken: 0.015628 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "V_policy, policy_policy = policy_iteration(initial_policy.copy(), env, gamma, threshold, max_iterations,num_of_actions,num_of_states)\n",
    "policy_time = time.time() - start_time\n",
    "wins_policy, avg_return_policy = evaluate_policy(env, policy_policy, num_episodes=1000)\n",
    "\n",
    "print(\"Policy Iteration Results:\")\n",
    "print(f\"Wins: {wins_policy}/1000 episodes\")\n",
    "print(f\"Average Return: {avg_return_policy:.3f}\")\n",
    "print(f\"Time Taken: {policy_time:.6f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "V_value, policy_value = value_iteration(env, gamma, threshold, max_iterations,num_of_states,num_of_actions)\n",
    "value_time = time.time() - start_time\n",
    "wins_value, avg_return_value = evaluate_policy(env, policy_value, num_episodes=1000)\n",
    "\n",
    "print(\"\\nValue Iteration Results:\")\n",
    "print(f\"Wins: {wins_value}/1000 episodes\")\n",
    "print(f\"Average Return: {avg_return_value:.3f}\")\n",
    "print(f\"Time Taken: {value_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b424f6b",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "- **Value Iteration** performed better with **more wins (742 vs. 704)** and **higher average return**.\n",
    "- **Value Iteration was faster**, converging in **less time** than Policy Iteration.\n",
    "\n",
    "ðŸ”¹ **Final Verdict**: **Value Iteration** is more efficient in this scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
