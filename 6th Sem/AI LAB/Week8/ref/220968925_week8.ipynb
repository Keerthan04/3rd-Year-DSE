{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8ae147-c8ba-4243-ba73-c571f74b6630",
   "metadata": {},
   "source": [
    "# Santhosh Prabhu\n",
    "# 220968025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea899461-0d38-4dd3-98e8-d61f4a8d0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c0709f-a976-4f3b-91ed-3e1144359e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(strategy, environment, gamma=0.99, threshold=1e-8, max_iters=1000):\n",
    "\n",
    "    if hasattr(environment, 'unwrapped'):\n",
    "        environment = environment.unwrapped\n",
    "\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "    value_func = np.zeros(num_states)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(num_states):\n",
    "                old_value = value_func[state]\n",
    "                new_value = 0\n",
    "                for action, action_prob in enumerate(strategy[state]):\n",
    "                    for prob, next_state, reward, done in environment.P[state][action]:\n",
    "                        new_value += action_prob * prob * (reward + gamma * value_func[next_state])\n",
    "                value_func[state] = new_value\n",
    "                delta = max(delta, abs(old_value - new_value))\n",
    "            if delta < threshold:\n",
    "                break\n",
    "\n",
    "        stable = True\n",
    "        for state in range(num_states):\n",
    "            old_action = np.argmax(strategy[state])\n",
    "            action_values = np.zeros(num_actions)\n",
    "            for action in range(num_actions):\n",
    "                for prob, next_state, reward, done in environment.P[state][action]:\n",
    "                    action_values[action] += prob * (reward + gamma * value_func[next_state])\n",
    "            best_action = np.argmax(action_values)\n",
    "            new_strategy = np.eye(num_actions)[best_action]\n",
    "            if not np.array_equal(new_strategy, strategy[state]):\n",
    "                stable = False\n",
    "            strategy[state] = new_strategy\n",
    "        \n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    return value_func, strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "198d0272-3f83-4841-b2f1-0ae0c379c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, gamma=0.99, threshold=1e-8, max_iters=1000):\n",
    "\n",
    "    if hasattr(environment, 'unwrapped'):\n",
    "        environment = environment.unwrapped\n",
    "\n",
    "    num_states = environment.observation_space.n\n",
    "    num_actions = environment.action_space.n\n",
    "    value_func = np.zeros(num_states)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0\n",
    "        for state in range(num_states):\n",
    "            old_value = value_func[state]\n",
    "            action_vals = np.zeros(num_actions)\n",
    "            for action in range(num_actions):\n",
    "                for prob, next_state, reward, done in environment.P[state][action]:\n",
    "                    action_vals[action] += prob * (reward + gamma * value_func[next_state])\n",
    "            value_func[state] = np.max(action_vals)\n",
    "            delta = max(delta, abs(old_value - value_func[state]))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    strategy = np.zeros((num_states, num_actions))\n",
    "    for state in range(num_states):\n",
    "        action_vals = np.zeros(num_actions)\n",
    "        for action in range(num_actions):\n",
    "            for prob, next_state, reward, done in environment.P[state][action]:\n",
    "                action_vals[action] += prob * (reward + gamma * value_func[next_state])\n",
    "        best_action = np.argmax(action_vals)\n",
    "        strategy[state] = np.eye(num_actions)[best_action]\n",
    "\n",
    "    return value_func, strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ba8727-73d7-44cb-aa8b-6b1c58b95695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_strategy(environment, strategy, episodes=1000):\n",
    "\n",
    "    successes = 0\n",
    "    total_rewards = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = environment.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(np.arange(environment.action_space.n), p=strategy[state])\n",
    "            state, reward, done, _, _ = environment.step(action)\n",
    "            episode_reward += reward\n",
    "        if reward > 0:\n",
    "            successes += 1\n",
    "        total_rewards += episode_reward\n",
    "    return successes, total_rewards / episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d92d9d4d-3fe1-4f15-a59b-59931a322ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Results:\n",
      "Wins: 821 / 1000 episodes\n",
      "Average Return: 0.821\n",
      "\n",
      "Value Iteration Results:\n",
      "Wins: 808 / 1000 episodes\n",
      "Average Return: 0.808\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize environment\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initial strategy: equal probability for all actions\n",
    "    init_strategy = np.ones((num_states, num_actions)) / num_actions\n",
    "    gamma = 0.99\n",
    "    threshold = 1e-8\n",
    "    max_iters = 1000\n",
    "\n",
    "    # Apply Policy Iteration\n",
    "    values_pi, strategy_pi = policy_iteration(init_strategy.copy(), env, gamma, threshold, max_iters)\n",
    "    wins_pi, avg_return_pi = assess_strategy(env, strategy_pi, episodes=1000)\n",
    "    print(\"Policy Iteration Results:\")\n",
    "    print(f\"Wins: {wins_pi} / 1000 episodes\")\n",
    "    print(f\"Average Return: {avg_return_pi:.3f}\")\n",
    "\n",
    "    # Apply Value Iteration\n",
    "    values_vi, strategy_vi = value_iteration(env, gamma, threshold, max_iters)\n",
    "    wins_vi, avg_return_vi = assess_strategy(env, strategy_vi, episodes=1000)\n",
    "    print(\"\\nValue Iteration Results:\")\n",
    "    print(f\"Wins: {wins_vi} / 1000 episodes\")\n",
    "    print(f\"Average Return: {avg_return_vi:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e4a0e-ba59-4e4c-a508-481ddc65f22a",
   "metadata": {},
   "source": [
    "While both methods performed well, Policy Iteration outperformed Value Iteration in this experiment, making it the preferred choice for solving this FrozenLake-v1 problem. However, in environments with a large state space, Value Iteration may be preferred due to its computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb55af-4175-4dd0-961b-ef3c3cc68201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
