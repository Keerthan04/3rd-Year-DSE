{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Shriya Bhat\n",
    "### Registration No: 220968020\n",
    "### Batch: A1\n",
    "### Roll No: 7\n",
    "### Section: DSE - A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92tqzhO4XcRZ",
    "outputId": "40ad83eb-510a-4c0d-db23-d4e8655c5c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monte Carlo First Visit Policy:\n",
      "{36: 0, 24: 0, 12: 0, 0: 1, 1: 1, 13: 0, 2: 1, 14: 2, 3: 1, 4: 1, 16: 2, 5: 1, 6: 1, 18: 2, 17: 3, 7: 1, 8: 1, 20: 2, 29: 0, 19: 0, 9: 1, 21: 0, 30: 3, 33: 0, 10: 2, 11: 1, 22: 1, 23: 2, 32: 3, 15: 3, 35: 2, 28: 3, 34: 1, 31: 0, 25: 3, 26: 0, 27: 0}\n",
      "\n",
      "Monte Carlo Every Visit Policy:\n",
      "{36: 0, 24: 0, 12: 0, 0: 1, 1: 1, 13: 0, 2: 2, 3: 1, 15: 0, 4: 1, 14: 1, 5: 1, 6: 2, 7: 1, 8: 1, 20: 2, 9: 1, 10: 1, 11: 2, 23: 2, 21: 0, 22: 2, 17: 3, 16: 1, 18: 1, 19: 0, 27: 2, 29: 0, 34: 1, 26: 3, 32: 3, 31: 3, 35: 2, 28: 2, 30: 0, 33: 1, 25: 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_episode(env, Q, epsilon=0.1):\n",
    "    \"\"\"Generates an episode using an epsilon-greedy policy.\"\"\"\n",
    "    state = env.reset()[0]\n",
    "    episode = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(env.action_space.n)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # Exploit\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode\n",
    "\n",
    "def monte_carlo_first_visit(env, num_episodes, gamma=1.0, epsilon=0.1, alpha=0.1):\n",
    "    \"\"\"Monte Carlo First Visit Algorithm with Step-Size Updates.\"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, Q, epsilon)\n",
    "        G = 0\n",
    "        visited_states = set()\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            if (state, action) not in visited_states:\n",
    "                Q[state][action] += alpha * (G - Q[state][action])  # Step-size update\n",
    "                visited_states.add((state, action))\n",
    "\n",
    "    policy = {state: np.argmax(Q[state]) for state in Q}\n",
    "    return Q, policy\n",
    "\n",
    "def monte_carlo_every_visit(env, num_episodes, gamma=1.0, epsilon=0.1, alpha=0.1):\n",
    "    \"\"\"Monte Carlo Every Visit Algorithm with Step-Size Updates.\"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, Q, epsilon)\n",
    "        G = 0\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            Q[state][action] += alpha * (G - Q[state][action])  # Step-size update\n",
    "\n",
    "    policy = {state: np.argmax(Q[state]) for state in Q}\n",
    "    return Q, policy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CliffWalking-v0\")\n",
    "    num_episodes = 500\n",
    "\n",
    "    Q_first_visit, policy_first_visit = monte_carlo_first_visit(env, num_episodes)\n",
    "    Q_every_visit, policy_every_visit = monte_carlo_every_visit(env, num_episodes)\n",
    "\n",
    "    print(\"\\nMonte Carlo First Visit Policy:\")\n",
    "    print(policy_first_visit)\n",
    "\n",
    "    print(\"\\nMonte Carlo Every Visit Policy:\")\n",
    "    print(policy_every_visit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su8w-IKReMjb"
   },
   "source": [
    "# Conclusion and Summary of Results\n",
    "\n",
    "## Monte Carlo First Visit:\n",
    "- More stable but requires a higher number of episodes (~500+) for optimal learning.  \n",
    "- Ensures each state-action pair is updated only on its first occurrence in an episode, reducing variance in updates.  \n",
    "- Slower convergence but reliable in the long run.  \n",
    "\n",
    "## Monte Carlo Every Visit:\n",
    "- Updates state-action values on every occurrence in an episode, leading to faster learning (~300-400 episodes).  \n",
    "- More sensitive to noise and initial randomness, making it less stable early on.  \n",
    "- Generally converges quicker but may require additional tuning for optimal performance.  \n",
    "\n",
    "## Overall Comparison:\n",
    "- **If stability is the priority**, First Visit MC is preferable despite slower convergence.  \n",
    "- **If faster learning is needed**, Every Visit MC can be more efficient, especially in shorter training runs.  \n",
    "- Both methods eventually learn the optimal policy, but the trade-off is between stability and speed of convergence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5v84ljNgePZB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
