{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c77ee96-63d6-4eed-b6dd-6c819a6856a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "634575f9-385f-4c15-a517-08627dfd7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(sample_size, n_steps):\n",
    " freq1, freq2, offsets1, offsets2 = np.random.rand(4, sample_size, 1)\n",
    " time = np.linspace(0, 1, n_steps)\n",
    " series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) #wave1+\n",
    " series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) #wave2+\n",
    " series += 0.1 * (np.random.rand(sample_size, n_steps) - 0.5) #noise\n",
    " return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e16cdd1-71fa-4439-b3f7-fb68a99a08c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 51, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = generate_time_series(10000,51)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231bad67-baae-4e35-b7db-12cf3b5266c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.190458</td>\n",
       "      <td>-0.032284</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.067265</td>\n",
       "      <td>-0.002433</td>\n",
       "      <td>0.134113</td>\n",
       "      <td>0.262115</td>\n",
       "      <td>0.504889</td>\n",
       "      <td>0.670177</td>\n",
       "      <td>0.678037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149966</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.099562</td>\n",
       "      <td>0.140216</td>\n",
       "      <td>0.174947</td>\n",
       "      <td>0.160068</td>\n",
       "      <td>0.380162</td>\n",
       "      <td>0.534238</td>\n",
       "      <td>0.576939</td>\n",
       "      <td>0.597725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.625143</td>\n",
       "      <td>-0.609529</td>\n",
       "      <td>-0.402722</td>\n",
       "      <td>-0.106313</td>\n",
       "      <td>0.102359</td>\n",
       "      <td>0.390579</td>\n",
       "      <td>0.547903</td>\n",
       "      <td>0.551331</td>\n",
       "      <td>0.570748</td>\n",
       "      <td>0.478422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524131</td>\n",
       "      <td>-0.614335</td>\n",
       "      <td>-0.539757</td>\n",
       "      <td>-0.359832</td>\n",
       "      <td>-0.033464</td>\n",
       "      <td>0.228977</td>\n",
       "      <td>0.391539</td>\n",
       "      <td>0.614097</td>\n",
       "      <td>0.635608</td>\n",
       "      <td>0.652441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.475906</td>\n",
       "      <td>-0.538642</td>\n",
       "      <td>-0.498716</td>\n",
       "      <td>-0.417347</td>\n",
       "      <td>-0.345950</td>\n",
       "      <td>-0.209307</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>0.143498</td>\n",
       "      <td>0.184406</td>\n",
       "      <td>0.284955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361417</td>\n",
       "      <td>-0.545315</td>\n",
       "      <td>-0.609307</td>\n",
       "      <td>-0.668221</td>\n",
       "      <td>-0.705867</td>\n",
       "      <td>-0.512083</td>\n",
       "      <td>-0.443648</td>\n",
       "      <td>-0.174030</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.251476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.136476</td>\n",
       "      <td>-0.139583</td>\n",
       "      <td>-0.255348</td>\n",
       "      <td>-0.380028</td>\n",
       "      <td>-0.520038</td>\n",
       "      <td>-0.556833</td>\n",
       "      <td>-0.511218</td>\n",
       "      <td>-0.400423</td>\n",
       "      <td>-0.109808</td>\n",
       "      <td>0.092136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194355</td>\n",
       "      <td>-0.252328</td>\n",
       "      <td>-0.276218</td>\n",
       "      <td>-0.409688</td>\n",
       "      <td>-0.451068</td>\n",
       "      <td>-0.531425</td>\n",
       "      <td>-0.533022</td>\n",
       "      <td>-0.343262</td>\n",
       "      <td>-0.204435</td>\n",
       "      <td>0.100624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.589533</td>\n",
       "      <td>-0.713867</td>\n",
       "      <td>-0.667822</td>\n",
       "      <td>-0.489073</td>\n",
       "      <td>-0.289055</td>\n",
       "      <td>-0.090757</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.143240</td>\n",
       "      <td>0.200832</td>\n",
       "      <td>0.218570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323978</td>\n",
       "      <td>-0.341098</td>\n",
       "      <td>-0.311707</td>\n",
       "      <td>-0.308957</td>\n",
       "      <td>-0.355592</td>\n",
       "      <td>-0.438151</td>\n",
       "      <td>-0.370953</td>\n",
       "      <td>-0.342154</td>\n",
       "      <td>-0.147752</td>\n",
       "      <td>0.109601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.408366</td>\n",
       "      <td>-0.527345</td>\n",
       "      <td>-0.646617</td>\n",
       "      <td>-0.657487</td>\n",
       "      <td>-0.541014</td>\n",
       "      <td>-0.403594</td>\n",
       "      <td>-0.196057</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>0.265527</td>\n",
       "      <td>0.380842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177727</td>\n",
       "      <td>-0.108182</td>\n",
       "      <td>-0.063159</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>0.163129</td>\n",
       "      <td>0.192304</td>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.318861</td>\n",
       "      <td>0.341726</td>\n",
       "      <td>0.328307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.228349</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>0.466534</td>\n",
       "      <td>0.464777</td>\n",
       "      <td>0.344801</td>\n",
       "      <td>0.219135</td>\n",
       "      <td>0.134694</td>\n",
       "      <td>-0.068955</td>\n",
       "      <td>-0.159522</td>\n",
       "      <td>-0.239312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150265</td>\n",
       "      <td>-0.075019</td>\n",
       "      <td>-0.225583</td>\n",
       "      <td>-0.341059</td>\n",
       "      <td>-0.526204</td>\n",
       "      <td>-0.497449</td>\n",
       "      <td>-0.505474</td>\n",
       "      <td>-0.319882</td>\n",
       "      <td>-0.170457</td>\n",
       "      <td>0.015890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.281785</td>\n",
       "      <td>0.113810</td>\n",
       "      <td>-0.055190</td>\n",
       "      <td>-0.264085</td>\n",
       "      <td>-0.361315</td>\n",
       "      <td>-0.390765</td>\n",
       "      <td>-0.415020</td>\n",
       "      <td>-0.342369</td>\n",
       "      <td>-0.319955</td>\n",
       "      <td>-0.330077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119893</td>\n",
       "      <td>0.180225</td>\n",
       "      <td>0.255683</td>\n",
       "      <td>0.293405</td>\n",
       "      <td>0.455743</td>\n",
       "      <td>0.478178</td>\n",
       "      <td>0.504441</td>\n",
       "      <td>0.465705</td>\n",
       "      <td>0.403102</td>\n",
       "      <td>0.281567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.367942</td>\n",
       "      <td>0.185129</td>\n",
       "      <td>-0.124268</td>\n",
       "      <td>-0.484930</td>\n",
       "      <td>-0.641957</td>\n",
       "      <td>-0.696576</td>\n",
       "      <td>-0.590067</td>\n",
       "      <td>-0.460997</td>\n",
       "      <td>-0.193649</td>\n",
       "      <td>0.145380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.282427</td>\n",
       "      <td>0.429823</td>\n",
       "      <td>0.540622</td>\n",
       "      <td>0.575259</td>\n",
       "      <td>0.462353</td>\n",
       "      <td>0.316299</td>\n",
       "      <td>0.089685</td>\n",
       "      <td>-0.059020</td>\n",
       "      <td>-0.203410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.338932</td>\n",
       "      <td>-0.411192</td>\n",
       "      <td>-0.559781</td>\n",
       "      <td>-0.562934</td>\n",
       "      <td>-0.564320</td>\n",
       "      <td>-0.409216</td>\n",
       "      <td>-0.183939</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.333944</td>\n",
       "      <td>0.427295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148049</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>-0.163816</td>\n",
       "      <td>-0.300108</td>\n",
       "      <td>-0.210199</td>\n",
       "      <td>-0.247630</td>\n",
       "      <td>-0.284469</td>\n",
       "      <td>-0.335918</td>\n",
       "      <td>-0.389800</td>\n",
       "      <td>-0.520615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.190458 -0.032284  0.055496  0.067265 -0.002433  0.134113  0.262115   \n",
       "1    -0.625143 -0.609529 -0.402722 -0.106313  0.102359  0.390579  0.547903   \n",
       "2    -0.475906 -0.538642 -0.498716 -0.417347 -0.345950 -0.209307 -0.021985   \n",
       "3    -0.136476 -0.139583 -0.255348 -0.380028 -0.520038 -0.556833 -0.511218   \n",
       "4    -0.589533 -0.713867 -0.667822 -0.489073 -0.289055 -0.090757  0.012367   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995 -0.408366 -0.527345 -0.646617 -0.657487 -0.541014 -0.403594 -0.196057   \n",
       "9996  0.228349  0.411773  0.466534  0.464777  0.344801  0.219135  0.134694   \n",
       "9997  0.281785  0.113810 -0.055190 -0.264085 -0.361315 -0.390765 -0.415020   \n",
       "9998  0.367942  0.185129 -0.124268 -0.484930 -0.641957 -0.696576 -0.590067   \n",
       "9999 -0.338932 -0.411192 -0.559781 -0.562934 -0.564320 -0.409216 -0.183939   \n",
       "\n",
       "            7         8         9   ...        41        42        43  \\\n",
       "0     0.504889  0.670177  0.678037  ... -0.149966  0.031481  0.099562   \n",
       "1     0.551331  0.570748  0.478422  ... -0.524131 -0.614335 -0.539757   \n",
       "2     0.143498  0.184406  0.284955  ... -0.361417 -0.545315 -0.609307   \n",
       "3    -0.400423 -0.109808  0.092136  ... -0.194355 -0.252328 -0.276218   \n",
       "4     0.143240  0.200832  0.218570  ... -0.323978 -0.341098 -0.311707   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.006313  0.265527  0.380842  ... -0.177727 -0.108182 -0.063159   \n",
       "9996 -0.068955 -0.159522 -0.239312  ...  0.150265 -0.075019 -0.225583   \n",
       "9997 -0.342369 -0.319955 -0.330077  ...  0.119893  0.180225  0.255683   \n",
       "9998 -0.460997 -0.193649  0.145380  ... -0.000106  0.282427  0.429823   \n",
       "9999  0.018688  0.333944  0.427295  ...  0.148049 -0.030072 -0.163816   \n",
       "\n",
       "            44        45        46        47        48        49        50  \n",
       "0     0.140216  0.174947  0.160068  0.380162  0.534238  0.576939  0.597725  \n",
       "1    -0.359832 -0.033464  0.228977  0.391539  0.614097  0.635608  0.652441  \n",
       "2    -0.668221 -0.705867 -0.512083 -0.443648 -0.174030  0.036190  0.251476  \n",
       "3    -0.409688 -0.451068 -0.531425 -0.533022 -0.343262 -0.204435  0.100624  \n",
       "4    -0.308957 -0.355592 -0.438151 -0.370953 -0.342154 -0.147752  0.109601  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995  0.071566  0.163129  0.192304  0.262376  0.318861  0.341726  0.328307  \n",
       "9996 -0.341059 -0.526204 -0.497449 -0.505474 -0.319882 -0.170457  0.015890  \n",
       "9997  0.293405  0.455743  0.478178  0.504441  0.465705  0.403102  0.281567  \n",
       "9998  0.540622  0.575259  0.462353  0.316299  0.089685 -0.059020 -0.203410  \n",
       "9999 -0.300108 -0.210199 -0.247630 -0.284469 -0.335918 -0.389800 -0.520615  \n",
       "\n",
       "[10000 rows x 51 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset.squeeze())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37c582c-6232-4a79-93fa-439017d0abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c716ce5-12ac-453a-abec-6975c266ecd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.190458</td>\n",
       "      <td>-0.032284</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.067265</td>\n",
       "      <td>-0.002433</td>\n",
       "      <td>0.134113</td>\n",
       "      <td>0.262115</td>\n",
       "      <td>0.504889</td>\n",
       "      <td>0.670177</td>\n",
       "      <td>0.678037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365875</td>\n",
       "      <td>-0.149966</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.099562</td>\n",
       "      <td>0.140216</td>\n",
       "      <td>0.174947</td>\n",
       "      <td>0.160068</td>\n",
       "      <td>0.380162</td>\n",
       "      <td>0.534238</td>\n",
       "      <td>0.576939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.625143</td>\n",
       "      <td>-0.609529</td>\n",
       "      <td>-0.402722</td>\n",
       "      <td>-0.106313</td>\n",
       "      <td>0.102359</td>\n",
       "      <td>0.390579</td>\n",
       "      <td>0.547903</td>\n",
       "      <td>0.551331</td>\n",
       "      <td>0.570748</td>\n",
       "      <td>0.478422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425538</td>\n",
       "      <td>-0.524131</td>\n",
       "      <td>-0.614335</td>\n",
       "      <td>-0.539757</td>\n",
       "      <td>-0.359832</td>\n",
       "      <td>-0.033464</td>\n",
       "      <td>0.228977</td>\n",
       "      <td>0.391539</td>\n",
       "      <td>0.614097</td>\n",
       "      <td>0.635608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.475906</td>\n",
       "      <td>-0.538642</td>\n",
       "      <td>-0.498716</td>\n",
       "      <td>-0.417347</td>\n",
       "      <td>-0.345950</td>\n",
       "      <td>-0.209307</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>0.143498</td>\n",
       "      <td>0.184406</td>\n",
       "      <td>0.284955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132401</td>\n",
       "      <td>-0.361417</td>\n",
       "      <td>-0.545315</td>\n",
       "      <td>-0.609307</td>\n",
       "      <td>-0.668221</td>\n",
       "      <td>-0.705867</td>\n",
       "      <td>-0.512083</td>\n",
       "      <td>-0.443648</td>\n",
       "      <td>-0.174030</td>\n",
       "      <td>0.036190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.136476</td>\n",
       "      <td>-0.139583</td>\n",
       "      <td>-0.255348</td>\n",
       "      <td>-0.380028</td>\n",
       "      <td>-0.520038</td>\n",
       "      <td>-0.556833</td>\n",
       "      <td>-0.511218</td>\n",
       "      <td>-0.400423</td>\n",
       "      <td>-0.109808</td>\n",
       "      <td>0.092136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178937</td>\n",
       "      <td>-0.194355</td>\n",
       "      <td>-0.252328</td>\n",
       "      <td>-0.276218</td>\n",
       "      <td>-0.409688</td>\n",
       "      <td>-0.451068</td>\n",
       "      <td>-0.531425</td>\n",
       "      <td>-0.533022</td>\n",
       "      <td>-0.343262</td>\n",
       "      <td>-0.204435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.589533</td>\n",
       "      <td>-0.713867</td>\n",
       "      <td>-0.667822</td>\n",
       "      <td>-0.489073</td>\n",
       "      <td>-0.289055</td>\n",
       "      <td>-0.090757</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.143240</td>\n",
       "      <td>0.200832</td>\n",
       "      <td>0.218570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362480</td>\n",
       "      <td>-0.323978</td>\n",
       "      <td>-0.341098</td>\n",
       "      <td>-0.311707</td>\n",
       "      <td>-0.308957</td>\n",
       "      <td>-0.355592</td>\n",
       "      <td>-0.438151</td>\n",
       "      <td>-0.370953</td>\n",
       "      <td>-0.342154</td>\n",
       "      <td>-0.147752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.408366</td>\n",
       "      <td>-0.527345</td>\n",
       "      <td>-0.646617</td>\n",
       "      <td>-0.657487</td>\n",
       "      <td>-0.541014</td>\n",
       "      <td>-0.403594</td>\n",
       "      <td>-0.196057</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>0.265527</td>\n",
       "      <td>0.380842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230264</td>\n",
       "      <td>-0.177727</td>\n",
       "      <td>-0.108182</td>\n",
       "      <td>-0.063159</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>0.163129</td>\n",
       "      <td>0.192304</td>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.318861</td>\n",
       "      <td>0.341726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.228349</td>\n",
       "      <td>0.411773</td>\n",
       "      <td>0.466534</td>\n",
       "      <td>0.464777</td>\n",
       "      <td>0.344801</td>\n",
       "      <td>0.219135</td>\n",
       "      <td>0.134694</td>\n",
       "      <td>-0.068955</td>\n",
       "      <td>-0.159522</td>\n",
       "      <td>-0.239312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290224</td>\n",
       "      <td>0.150265</td>\n",
       "      <td>-0.075019</td>\n",
       "      <td>-0.225583</td>\n",
       "      <td>-0.341059</td>\n",
       "      <td>-0.526204</td>\n",
       "      <td>-0.497449</td>\n",
       "      <td>-0.505474</td>\n",
       "      <td>-0.319882</td>\n",
       "      <td>-0.170457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.281785</td>\n",
       "      <td>0.113810</td>\n",
       "      <td>-0.055190</td>\n",
       "      <td>-0.264085</td>\n",
       "      <td>-0.361315</td>\n",
       "      <td>-0.390765</td>\n",
       "      <td>-0.415020</td>\n",
       "      <td>-0.342369</td>\n",
       "      <td>-0.319955</td>\n",
       "      <td>-0.330077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074934</td>\n",
       "      <td>0.119893</td>\n",
       "      <td>0.180225</td>\n",
       "      <td>0.255683</td>\n",
       "      <td>0.293405</td>\n",
       "      <td>0.455743</td>\n",
       "      <td>0.478178</td>\n",
       "      <td>0.504441</td>\n",
       "      <td>0.465705</td>\n",
       "      <td>0.403102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.367942</td>\n",
       "      <td>0.185129</td>\n",
       "      <td>-0.124268</td>\n",
       "      <td>-0.484930</td>\n",
       "      <td>-0.641957</td>\n",
       "      <td>-0.696576</td>\n",
       "      <td>-0.590067</td>\n",
       "      <td>-0.460997</td>\n",
       "      <td>-0.193649</td>\n",
       "      <td>0.145380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333715</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>0.282427</td>\n",
       "      <td>0.429823</td>\n",
       "      <td>0.540622</td>\n",
       "      <td>0.575259</td>\n",
       "      <td>0.462353</td>\n",
       "      <td>0.316299</td>\n",
       "      <td>0.089685</td>\n",
       "      <td>-0.059020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.338932</td>\n",
       "      <td>-0.411192</td>\n",
       "      <td>-0.559781</td>\n",
       "      <td>-0.562934</td>\n",
       "      <td>-0.564320</td>\n",
       "      <td>-0.409216</td>\n",
       "      <td>-0.183939</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.333944</td>\n",
       "      <td>0.427295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342986</td>\n",
       "      <td>0.148049</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>-0.163816</td>\n",
       "      <td>-0.300108</td>\n",
       "      <td>-0.210199</td>\n",
       "      <td>-0.247630</td>\n",
       "      <td>-0.284469</td>\n",
       "      <td>-0.335918</td>\n",
       "      <td>-0.389800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.190458 -0.032284  0.055496  0.067265 -0.002433  0.134113  0.262115   \n",
       "1    -0.625143 -0.609529 -0.402722 -0.106313  0.102359  0.390579  0.547903   \n",
       "2    -0.475906 -0.538642 -0.498716 -0.417347 -0.345950 -0.209307 -0.021985   \n",
       "3    -0.136476 -0.139583 -0.255348 -0.380028 -0.520038 -0.556833 -0.511218   \n",
       "4    -0.589533 -0.713867 -0.667822 -0.489073 -0.289055 -0.090757  0.012367   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995 -0.408366 -0.527345 -0.646617 -0.657487 -0.541014 -0.403594 -0.196057   \n",
       "9996  0.228349  0.411773  0.466534  0.464777  0.344801  0.219135  0.134694   \n",
       "9997  0.281785  0.113810 -0.055190 -0.264085 -0.361315 -0.390765 -0.415020   \n",
       "9998  0.367942  0.185129 -0.124268 -0.484930 -0.641957 -0.696576 -0.590067   \n",
       "9999 -0.338932 -0.411192 -0.559781 -0.562934 -0.564320 -0.409216 -0.183939   \n",
       "\n",
       "            7         8         9   ...        40        41        42  \\\n",
       "0     0.504889  0.670177  0.678037  ... -0.365875 -0.149966  0.031481   \n",
       "1     0.551331  0.570748  0.478422  ... -0.425538 -0.524131 -0.614335   \n",
       "2     0.143498  0.184406  0.284955  ... -0.132401 -0.361417 -0.545315   \n",
       "3    -0.400423 -0.109808  0.092136  ... -0.178937 -0.194355 -0.252328   \n",
       "4     0.143240  0.200832  0.218570  ... -0.362480 -0.323978 -0.341098   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.006313  0.265527  0.380842  ... -0.230264 -0.177727 -0.108182   \n",
       "9996 -0.068955 -0.159522 -0.239312  ...  0.290224  0.150265 -0.075019   \n",
       "9997 -0.342369 -0.319955 -0.330077  ...  0.074934  0.119893  0.180225   \n",
       "9998 -0.460997 -0.193649  0.145380  ... -0.333715 -0.000106  0.282427   \n",
       "9999  0.018688  0.333944  0.427295  ...  0.342986  0.148049 -0.030072   \n",
       "\n",
       "            43        44        45        46        47        48        49  \n",
       "0     0.099562  0.140216  0.174947  0.160068  0.380162  0.534238  0.576939  \n",
       "1    -0.539757 -0.359832 -0.033464  0.228977  0.391539  0.614097  0.635608  \n",
       "2    -0.609307 -0.668221 -0.705867 -0.512083 -0.443648 -0.174030  0.036190  \n",
       "3    -0.276218 -0.409688 -0.451068 -0.531425 -0.533022 -0.343262 -0.204435  \n",
       "4    -0.311707 -0.308957 -0.355592 -0.438151 -0.370953 -0.342154 -0.147752  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995 -0.063159  0.071566  0.163129  0.192304  0.262376  0.318861  0.341726  \n",
       "9996 -0.225583 -0.341059 -0.526204 -0.497449 -0.505474 -0.319882 -0.170457  \n",
       "9997  0.255683  0.293405  0.455743  0.478178  0.504441  0.465705  0.403102  \n",
       "9998  0.429823  0.540622  0.575259  0.462353  0.316299  0.089685 -0.059020  \n",
       "9999 -0.163816 -0.300108 -0.210199 -0.247630 -0.284469 -0.335918 -0.389800  \n",
       "\n",
       "[10000 rows x 50 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1437059-34ce-49c3-8b1e-d8d290964809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1937ed06-d8d9-4780-af6f-dcfcda8877f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN,Dense,Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c058a408-3a91-4aea-8550-cbe781d1d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:33:14.962679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.078703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.079106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.083650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.083990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.084219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.640031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.640158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.640227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 14:33:15.640301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9992 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "fcnn_model = Sequential([\n",
    "    Input(shape=(50,),name='input'),\n",
    "    Dense(128, activation='relu',name='d1'),\n",
    "    Dense(64, activation='relu',name='d2'),\n",
    "    Dense(32, activation='relu',name='d3'), \n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84da89e7-8dd5-49e8-b3f0-b1cf58733b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = Sequential([\n",
    "    SimpleRNN(units = 50, return_sequences = True, input_shape = (50,1)),\n",
    "    Dense(units = 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fed78e1-c71e-47d2-a601-2770a3860464",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = Sequential([\n",
    "    SimpleRNN(units = 50, return_sequences = True, input_shape = (50,1)),\n",
    "    SimpleRNN(units = 50, return_sequences = True, input_shape = (50,1)),\n",
    "    Dense(units = 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2aa542e-5060-4970-9710-1ab553f884e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: fcnn\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " d1 (Dense)                  (None, 128)               6528      \n",
      "                                                                 \n",
      " d2 (Dense)                  (None, 64)                8256      \n",
      "                                                                 \n",
      " d3 (Dense)                  (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16897 (66.00 KB)\n",
      "Trainable params: 16897 (66.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "219/219 [==============================] - 2s 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 2/25\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 3/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 4/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 5/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 6/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 7/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 8/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 9/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 10/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 11/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 12/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 13/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 14/25\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 15/25\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 16/25\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 17/25\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 18/25\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 19/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 20/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 21/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 22/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 23/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 24/25\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 25/25\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "Training model: rnn1\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_3 (SimpleRNN)    (None, 50, 50)            2600      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50, 1)             51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2651 (10.36 KB)\n",
      "Trainable params: 2651 (10.36 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0727 - mean_squared_error: 0.0727 - val_loss: 0.0675 - val_mean_squared_error: 0.0675\n",
      "Epoch 2/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0713 - mean_squared_error: 0.0713 - val_loss: 0.0662 - val_mean_squared_error: 0.0662\n",
      "Epoch 3/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0710 - mean_squared_error: 0.0710 - val_loss: 0.0667 - val_mean_squared_error: 0.0667\n",
      "Epoch 4/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0710 - mean_squared_error: 0.0710 - val_loss: 0.0684 - val_mean_squared_error: 0.0684\n",
      "Epoch 5/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0709 - mean_squared_error: 0.0709 - val_loss: 0.0678 - val_mean_squared_error: 0.0678\n",
      "Epoch 6/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0708 - mean_squared_error: 0.0708 - val_loss: 0.0652 - val_mean_squared_error: 0.0652\n",
      "Epoch 7/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0698 - mean_squared_error: 0.0698 - val_loss: 0.0768 - val_mean_squared_error: 0.0768\n",
      "Epoch 8/25\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0698 - mean_squared_error: 0.0698 - val_loss: 0.0659 - val_mean_squared_error: 0.0659\n",
      "Epoch 9/25\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0698 - mean_squared_error: 0.0698 - val_loss: 0.0678 - val_mean_squared_error: 0.0678\n",
      "Epoch 10/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0690 - mean_squared_error: 0.0690 - val_loss: 0.0673 - val_mean_squared_error: 0.0673\n",
      "Epoch 11/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0691 - mean_squared_error: 0.0691 - val_loss: 0.0704 - val_mean_squared_error: 0.0704\n",
      "Epoch 12/25\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0692 - mean_squared_error: 0.0692 - val_loss: 0.0699 - val_mean_squared_error: 0.0699\n",
      "Epoch 13/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0687 - mean_squared_error: 0.0687 - val_loss: 0.0672 - val_mean_squared_error: 0.0672\n",
      "Epoch 14/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0694 - mean_squared_error: 0.0694 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "Epoch 15/25\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0687 - mean_squared_error: 0.0687 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "Epoch 16/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0680 - val_mean_squared_error: 0.0680\n",
      "Epoch 17/25\n",
      "219/219 [==============================] - 3s 11ms/step - loss: 0.0682 - mean_squared_error: 0.0682 - val_loss: 0.0672 - val_mean_squared_error: 0.0672\n",
      "Epoch 18/25\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0669 - val_mean_squared_error: 0.0669\n",
      "Epoch 19/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0680 - mean_squared_error: 0.0680 - val_loss: 0.0638 - val_mean_squared_error: 0.0638\n",
      "Epoch 20/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 21/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0675 - mean_squared_error: 0.0675 - val_loss: 0.0646 - val_mean_squared_error: 0.0646\n",
      "Epoch 22/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0668 - mean_squared_error: 0.0668 - val_loss: 0.0649 - val_mean_squared_error: 0.0649\n",
      "Epoch 23/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0673 - mean_squared_error: 0.0673 - val_loss: 0.0660 - val_mean_squared_error: 0.0660\n",
      "Epoch 24/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0677 - mean_squared_error: 0.0677 - val_loss: 0.0655 - val_mean_squared_error: 0.0655\n",
      "Epoch 25/25\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0666 - mean_squared_error: 0.0666 - val_loss: 0.0661 - val_mean_squared_error: 0.0661\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0678 - mean_squared_error: 0.0678\n",
      "Training model: rnn2\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, 50, 50)            2600      \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 50, 50)            5050      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50, 1)             51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7701 (30.08 KB)\n",
      "Trainable params: 7701 (30.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "219/219 [==============================] - 6s 22ms/step - loss: 0.0688 - mean_squared_error: 0.0688 - val_loss: 0.0669 - val_mean_squared_error: 0.0669\n",
      "Epoch 2/25\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0658 - val_mean_squared_error: 0.0658\n",
      "Epoch 3/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0676 - mean_squared_error: 0.0676 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 4/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0671 - mean_squared_error: 0.0671 - val_loss: 0.0672 - val_mean_squared_error: 0.0672\n",
      "Epoch 5/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0670 - mean_squared_error: 0.0670 - val_loss: 0.0622 - val_mean_squared_error: 0.0622\n",
      "Epoch 6/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0668 - mean_squared_error: 0.0668 - val_loss: 0.0666 - val_mean_squared_error: 0.0666\n",
      "Epoch 7/25\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0667 - mean_squared_error: 0.0667 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 8/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0665 - mean_squared_error: 0.0665 - val_loss: 0.0627 - val_mean_squared_error: 0.0627\n",
      "Epoch 9/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0655 - mean_squared_error: 0.0655 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 10/25\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0662 - mean_squared_error: 0.0662 - val_loss: 0.0622 - val_mean_squared_error: 0.0622\n",
      "Epoch 11/25\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0673 - mean_squared_error: 0.0673 - val_loss: 0.0652 - val_mean_squared_error: 0.0652\n",
      "Epoch 12/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0650 - mean_squared_error: 0.0650 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 13/25\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0654 - mean_squared_error: 0.0654 - val_loss: 0.0618 - val_mean_squared_error: 0.0618\n",
      "Epoch 14/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0651 - mean_squared_error: 0.0651 - val_loss: 0.0631 - val_mean_squared_error: 0.0631\n",
      "Epoch 15/25\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0642 - mean_squared_error: 0.0642 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 16/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0645 - mean_squared_error: 0.0645 - val_loss: 0.0618 - val_mean_squared_error: 0.0618\n",
      "Epoch 17/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0650 - mean_squared_error: 0.0650 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 18/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0640 - mean_squared_error: 0.0640 - val_loss: 0.0621 - val_mean_squared_error: 0.0621\n",
      "Epoch 19/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0636 - mean_squared_error: 0.0636 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 20/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0637 - mean_squared_error: 0.0637 - val_loss: 0.0627 - val_mean_squared_error: 0.0627\n",
      "Epoch 21/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0633 - mean_squared_error: 0.0633 - val_loss: 0.0623 - val_mean_squared_error: 0.0623\n",
      "Epoch 22/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0635 - mean_squared_error: 0.0635 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 23/25\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0636 - mean_squared_error: 0.0636 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 24/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0624 - mean_squared_error: 0.0624 - val_loss: 0.0621 - val_mean_squared_error: 0.0621\n",
      "Epoch 25/25\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0622 - mean_squared_error: 0.0622 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0611 - mean_squared_error: 0.0611\n"
     ]
    }
   ],
   "source": [
    "models = {\"fcnn\":fcnn_model,\"rnn1\":rnn1,\"rnn2\":rnn2}\n",
    "def model_train(models, x_train, x_test, y_train, y_test):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training model: {model_name}\")\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "        print(model.summary())\n",
    "        history = model.fit(x_train, y_train, epochs=25, batch_size=32, validation_split=2/9)\n",
    "        test_loss, test_mse = model.evaluate(x_test, y_test)\n",
    "        \n",
    "        # Append results as a dictionary\n",
    "        results.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"mse\": history.history['mean_squared_error'][-1],\n",
    "            \"test_mse\": test_mse\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "results_df = model_train(models, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933341a0-990f-492e-b5c0-48cea40b5f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>mse</th>\n",
       "      <th>test_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fcnn</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.002282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn1</td>\n",
       "      <td>0.06658</td>\n",
       "      <td>0.067777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rnn2</td>\n",
       "      <td>0.06222</td>\n",
       "      <td>0.061057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name      mse  test_mse\n",
       "0       fcnn  0.00140  0.002282\n",
       "1       rnn1  0.06658  0.067777\n",
       "2       rnn2  0.06222  0.061057"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
